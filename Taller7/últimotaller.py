# -*- coding: utf-8 -*-
"""ÚltimoTaller.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JL2n8m2ezhJdPR24BFPI1x5iiBpg84Qw

# Distancias Encadenadas (Single Linkage)


1.   Definición del Algoritmo

  *   Single Linkage

      La distancia entre dos clusters se define como la distancia mínima entre cualquier par de puntos, donde cada punto pertenece a un cluster diferente.

  *   Proceso de Clustering

      Inicialmente, cada punto de datos se considera un cluster separado. En cada paso, los dos clusters más cercanos se combinan en un solo cluster. Este proceso se repite hasta que la distancia mínima entre clusters excede un umbral predefinido o se forma un solo cluster.

2.   Generación de Datos

  Los datos de muestra se generan con generate_data(n_samples=150, n_features=2, n_clusters=3), creando un conjunto de datos bidimensionales con tres clusters diferentes.

3.  Aplicación del Algoritmo



  *   Se calcula la distancia por pares (pairwise distances) entre todos los puntos de datos usando la distancia euclidiana.

  *   Se agrupan los puntos en clusters basándose en la distancia mínima entre ellos hasta que la distancia mínima excede un umbral definido (por ejemplo, threshold=10.0).

4. Resultados

*   El algoritmo produce una jerarquía de clusters. La visualización de los resultados muestra los clusters formados en una gráfica bidimensional.
*   Visualización: Utilizando Matplotlib, los clusters resultantes se visualizan en diferentes colores para identificar los grupos formados.


5. Conclusión

El algoritmo de Distancias Encadenadas (Single Linkage) es útil para identificar clusters en datos con formas irregulares y capturar la estructura de proximidad local. Sin embargo, es importante considerar sus limitaciones y ajustar parámetros como el umbral de distancia para obtener resultados significativos.
"""

!pip install opencv-python-headless
!pip install numpy scipy matplotlib

import numpy as np
import cv2
from scipy.spatial.distance import pdist, squareform
import matplotlib.pyplot as plt

# Generar datos de muestra para la demostración
def generate_data(n_samples=150, n_features=2, n_clusters=3):
    np.random.seed(42)
    X = []
    for i in range(n_clusters):
        cluster_center = np.random.rand(n_features) * 100
        X.append(cluster_center + np.random.randn(n_samples, n_features) * 10)
    return np.vstack(X)

# Algoritmo de Distancias Encadenadas (Single Linkage)
def single_linkage_clustering(X, threshold=10.0):
    pairwise_distances = squareform(pdist(X, 'euclidean'))
    clusters = {i: [i] for i in range(len(X))}

    while True:
        min_dist = float('inf')
        to_merge = None

        for i in clusters:
            for j in clusters:
                if i != j:
                    dist = min(pairwise_distances[p1, p2] for p1 in clusters[i] for p2 in clusters[j])
                    if dist < min_dist:
                        min_dist = dist
                        to_merge = (i, j)

        if min_dist > threshold:
            break

        i, j = to_merge
        clusters[i].extend(clusters[j])
        del clusters[j]

    return clusters

# Visualizar resultados
def plot_clusters(X, clusters, title):
    plt.figure(figsize=(10, 6))
    colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k']
    for i, cluster in clusters.items():
        cluster_points = X[cluster]
        plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors[i % len(colors)], label=f'Cluster {i}')
    plt.title(title)
    plt.legend()
    plt.show()

# Generar datos
X = generate_data()

# Aplicar y visualizar Single Linkage
clusters_single_linkage = single_linkage_clustering(X)
plot_clusters(X, clusters_single_linkage, 'Single Linkage Clustering')

"""---

# Max-Min Clustering

 1. Definición del Algoritmo



*   Max-Min Clustering

  Es un algoritmo de clustering no supervisado que selecciona puntos como centros iniciales basándose en la distancia máxima entre ellos y luego agrega iterativamente puntos al cluster cuyo centro está más cerca.

*   Proceso de Clustering

  - Selecciona el primer centro de manera aleatoria.
  - Selecciona el segundo centro como el punto más distante del primer centro.
  - Selecciona los siguientes centros basándose en la distancia máxima mínima de los puntos seleccionados.
  - Asigna cada punto al centro más cercano.

2. Generación de Datos

  Los datos de muestra se generan con generate_data(n_samples=150, n_features=2, n_clusters=3), creando un conjunto de datos bidimensionales con tres clusters diferentes.

3. Aplicación del Algoritmo

- Se calcula la distancia por pares (pairwise distances) entre todos los puntos de datos usando la distancia euclidiana.
- Se seleccionan los centros iniciales basándose en la distancia máxima entre ellos.
- Los puntos se asignan iterativamente al centro más cercano.

4. Resultados

- El algoritmo produce clusters basados en la proximidad a los centros seleccionados.
- Visualización: Utilizando Matplotlib, los clusters resultantes se visualizan en diferentes colores para identificar los grupos formados.

5. Conclusión

El algoritmo de Max-Min Clustering es útil para identificar clusters basados en la proximidad a centros seleccionados de manera estratégica. Es eficiente y puede proporcionar una buena separación inicial de los clusters. Sin embargo, su desempeño puede verse afectado por la elección de los centros iniciales y puede no capturar clusters con formas complejas. Ajustar el número de clusters y la selección inicial de centros puede ayudar a obtener resultados más precisos.
"""

import numpy as np
import cv2
from scipy.spatial.distance import pdist, squareform
import matplotlib.pyplot as plt

# Generar datos de muestra para la demostración
def generate_data(n_samples=150, n_features=2, n_clusters=3):
    np.random.seed(42)
    X = []
    for i in range(n_clusters):
        cluster_center = np.random.rand(n_features) * 100
        X.append(cluster_center + np.random.randn(n_samples, n_features) * 10)
    return np.vstack(X)

# Algoritmo Max-Min
def max_min_clustering(X, k=3):
    n_samples = X.shape[0]
    distances = squareform(pdist(X, 'euclidean'))

    # Seleccionar el primer centro aleatoriamente
    centers = [np.random.randint(n_samples)]
    # Seleccionar el segundo centro que esté a la distancia máxima del primer centro
    distances_to_first = distances[centers[0]]
    second_center = np.argmax(distances_to_first)
    centers.append(second_center)

    # Seleccionar los restantes centros
    for _ in range(2, k):
        min_distances = np.min(distances[centers], axis=0)
        next_center = np.argmax(min_distances)
        centers.append(next_center)

    clusters = {i: [] for i in range(k)}
    for idx, point in enumerate(X):
        closest_center = np.argmin([np.linalg.norm(point - X[center]) for center in centers])
        clusters[closest_center].append(idx)

    return clusters

# Visualizar resultados
def plot_clusters(X, clusters, title):
    plt.figure(figsize=(10, 6))
    colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k']
    for i, cluster in clusters.items():
        cluster_points = X[cluster]
        plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors[i % len(colors)], label=f'Cluster {i}')
    plt.title(title)
    plt.legend()
    plt.show()

# Generar datos
X = generate_data()

# Aplicar y visualizar Max-Min
clusters_max_min = max_min_clustering(X, k=3)
plot_clusters(X, clusters_max_min, 'Max-Min Clustering')

"""# Desempeños de los clasificadores aplicado al proyecto


---


"""

from google.colab import drive
drive.mount('/content/drive')

"""1. Montar Google Drive y Configurar Librerías"""

import numpy as np
import pandas as pd
import os
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

"""2. Configurar Generadores de Datos

"""

base_dir = '/content/drive/Shared drives/Técnicas IA/AVANCE 1/melanoma_cancer_dataset/test/'

# Crear generadores de datos con ImageDataGenerator
datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

train_generator = datagen.flow_from_directory(
    base_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode='binary',
    subset='training'
)

validation_generator = datagen.flow_from_directory(
    base_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode='binary',
    subset='validation'
)

"""3. Definir y Compilar el Modelo CNN

"""

model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),
    MaxPooling2D(pool_size=(2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

"""4. Entrenar el Modelo

"""

early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // 32,
    validation_steps=validation_generator.samples // 32,
    epochs=20,
    validation_data=validation_generator,
    callbacks=[early_stopping]
)

"""5. Evaluar el Modelo

"""

val_loss, val_accuracy = model.evaluate(validation_generator)
print(f'Validation Accuracy: {val_accuracy}')

# Generar predicciones
validation_generator.reset()
predictions = model.predict(validation_generator, steps=validation_generator.samples // 17)
predicted_classes = (predictions > 0.5).astype('int32').flatten()

true_classes = validation_generator.classes
class_labels = list(validation_generator.class_indices.keys())

# Imprimir el informe de clasificación y la matriz de confusión
print('Classification Report')
print(classification_report(true_classes, predicted_classes, target_names=class_labels))

print('Confusion Matrix')
print(confusion_matrix(true_classes, predicted_classes))

"""# Resultados del Modelo

Los resultados de tu modelo muestran una precisión general del 52%, con una precisión del 51% para la clase benigna y del 52% para la clase maligna. Aquí hay un análisis de los resultados:

### Precisión y Recuperación
- La precisión para la clase benigna es del 51%, lo que indica que el 51% de las muestras clasificadas como benignas son realmente benignas.
- La precisión para la clase maligna es del 52%, lo que indica que el 52% de las muestras clasificadas como malignas son realmente malignas.
- El recall para la clase benigna es del 60%, lo que indica que el modelo identificó correctamente el 60% de las muestras benignas.
- El recall para la clase maligna es del 43%, lo que indica que el modelo identificó correctamente el 43% de las muestras malignas.

### F1-Score
- El F1-score para la clase benigna es del 55%, que es una medida armónica entre precisión y recall.
- El F1-score para la clase maligna es del 47%, también una medida armónica entre precisión y recall.

### Matriz de Confusión
La matriz de confusión muestra que el modelo clasificó correctamente 60 muestras benignas y 43 muestras malignas. Sin embargo, también clasificó incorrectamente 40 muestras benignas como malignas y 57 muestras malignas como benignas.

### Análisis General
- La precisión general del modelo del 52% sugiere que el modelo puede estar teniendo dificultades para distinguir entre las clases.
- El recall relativamente bajo para la clase maligna indica que el modelo puede estar teniendo dificultades para identificar correctamente los casos malignos.
- La matriz de confusión muestra una cantidad significativa de muestras clasificadas incorrectamente, lo que sugiere que el modelo puede necesitar ajustes adicionales o un enfoque diferente para mejorar su desempeño.



"""